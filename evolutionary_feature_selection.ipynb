{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ebcbffa-5ec9-4df7-9380-ac2ceede0789",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/evolutionary-feature-selection-for-machine-learning-7f61af2a8c12\n",
    "\n",
    "### Evolutionary Feature Selection for Machine Learning\n",
    "In general, it’s not a good idea to use brute force approaches to optimize a model, in the case of feature selection, using methods like forward selection or backward elimination, which can only vary one feature at a time and tends to have troubles when it comes to seeing how different subsets (with the same size) of features work together.\n",
    "\n",
    "#### Model Representation:\n",
    "We can model the features as follows:\n",
    "- Each individual of the population represents the total subset of features.\n",
    "- The gen of the individual represents one particular feature.\n",
    "- Each gen value can be 0 or 1; zero means the algorithm did not select the feature, and one means the feature is included.\n",
    "- The mutation is associated with swamping the bit value in the randomly selected position within a mutation probability.\n",
    "\n",
    "#### Python Code:\n",
    "For this experiment, I’m going to use a classification dataset. Still, I’m also going to add random noise as new “garbage features” that are not useful for the model and add more complexity. I expect the model to remove them and possibly some of the originals. Hence, the first step is to import the data and create these new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d7dc19-c9bb-4a31-bc56-fe89834407e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 9)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data[\"data\"], data[\"target\"]\n",
    "\n",
    "# Add random non-important features\n",
    "noise = np.random.uniform(0, 10, size=(X.shape[0], 5))\n",
    "X = np.hstack((X, noise))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9dbc44-1ff5-4ccd-8b72-f4d5bf02200c",
   "metadata": {},
   "source": [
    "From the previous code, you can see there are nine features, four originals, and five dummies; we can plot them to check how they are related to the “y” variable, which we want to predict. Each color represents one of the categories.\n",
    "\n",
    "We can see that the original features help to discriminate the observations of each class having a boundary that separates them. Still, the new features (dummies) don’t add value since they cannot “split” the data per category, just as expected.\n",
    "\n",
    "Now, we will split the data into train and test and import the base model we want to use to select the features, in this case, a decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d7d11e-0827-49b6-b4c8-e8dfe2bbd414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f08eeb-382a-45ff-977d-641677fc0e04",
   "metadata": {},
   "source": [
    "As a next step, let’s import and fit the feature selection model; as mentioned, it uses evolutionary algorithms to select the features; it uses a multi-objective function by optimizing the cross-validation score while also minimizing the number of features used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145c7375-7f3b-4114-8e80-b9650d2b325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn-genetic-opt\n",
      "  Downloading sklearn_genetic_opt-0.7.0-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: tqdm>=4.61.1 in /Users/rafaelbaetacarreira/opt/miniconda3/lib/python3.8/site-packages (from sklearn-genetic-opt) (4.61.1)\n",
      "Collecting deap>=1.3.1\n",
      "  Downloading deap-1.3.1-cp38-cp38-macosx_10_14_x86_64.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /Users/rafaelbaetacarreira/opt/miniconda3/lib/python3.8/site-packages (from sklearn-genetic-opt) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/rafaelbaetacarreira/opt/miniconda3/lib/python3.8/site-packages (from sklearn-genetic-opt) (1.21.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/rafaelbaetacarreira/opt/miniconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->sklearn-genetic-opt) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rafaelbaetacarreira/opt/miniconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->sklearn-genetic-opt) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/rafaelbaetacarreira/opt/miniconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->sklearn-genetic-opt) (1.7.0)\n",
      "Installing collected packages: deap, sklearn-genetic-opt\n",
      "Successfully installed deap-1.3.1 sklearn-genetic-opt-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn-genetic-opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd91f14-7040-499e-8fbf-8cd020428e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tfitness \tfitness_std\tfitness_max\tfitness_min\n",
      "0  \t10    \t0.874667\t0.101008   \t0.946667   \t0.673333   \n",
      "1  \t14    \t0.934   \t0.0141264  \t0.966667   \t0.92       \n",
      "2  \t17    \t0.952   \t0.00884433 \t0.966667   \t0.94       \n",
      "3  \t18    \t0.954667\t0.00581187 \t0.96       \t0.946667   \n",
      "4  \t15    \t0.962667\t0.00442217 \t0.966667   \t0.953333   \n",
      "5  \t18    \t0.962   \t0.00426875 \t0.966667   \t0.953333   \n",
      "6  \t18    \t0.96    \t0.00788811 \t0.966667   \t0.94       \n",
      "7  \t18    \t0.962   \t0.00426875 \t0.966667   \t0.953333   \n",
      "8  \t14    \t0.965333\t0.00266667 \t0.966667   \t0.96       \n",
      "9  \t18    \t0.965333\t0.004      \t0.966667   \t0.953333   \n",
      "10 \t16    \t0.966   \t0.002      \t0.966667   \t0.96       \n",
      "11 \t19    \t0.965333\t0.004      \t0.966667   \t0.953333   \n",
      "12 \t19    \t0.966   \t0.002      \t0.966667   \t0.96       \n",
      "13 \t20    \t0.964667\t0.00305505 \t0.966667   \t0.96       \n",
      "14 \t18    \t0.964   \t0.00442217 \t0.966667   \t0.953333   \n",
      "15 \t18    \t0.964667\t0.006      \t0.966667   \t0.946667   \n",
      "16 \t19    \t0.966667\t1.11022e-16\t0.966667   \t0.966667   \n",
      "17 \t20    \t0.966667\t1.11022e-16\t0.966667   \t0.966667   \n",
      "18 \t19    \t0.963333\t0.01       \t0.966667   \t0.933333   \n",
      "19 \t19    \t0.966   \t0.002      \t0.966667   \t0.96       \n",
      "20 \t17    \t0.966667\t1.11022e-16\t0.966667   \t0.966667   \n",
      "21 \t19    \t0.962   \t0.00669992 \t0.966667   \t0.946667   \n",
      "22 \t18    \t0.964667\t0.00305505 \t0.966667   \t0.96       \n",
      "23 \t18    \t0.962   \t0.00520683 \t0.966667   \t0.953333   \n",
      "24 \t19    \t0.963333\t0.00447214 \t0.966667   \t0.953333   \n",
      "25 \t19    \t0.962667\t0.008      \t0.966667   \t0.946667   \n",
      "26 \t19    \t0.961333\t0.00933333 \t0.966667   \t0.94       \n",
      "27 \t17    \t0.964667\t0.006      \t0.966667   \t0.946667   \n",
      "28 \t18    \t0.964   \t0.00442217 \t0.966667   \t0.953333   \n",
      "29 \t20    \t0.966   \t0.002      \t0.966667   \t0.96       \n",
      "30 \t18    \t0.966   \t0.002      \t0.966667   \t0.96       \n",
      "31 \t19    \t0.962   \t0.00669992 \t0.966667   \t0.946667   \n",
      "32 \t16    \t0.964667\t0.00426875 \t0.966667   \t0.953333   \n",
      "33 \t16    \t0.966   \t0.00466667 \t0.973333   \t0.96       \n",
      "34 \t16    \t0.964667\t0.00733333 \t0.973333   \t0.946667   \n",
      "35 \t16    \t0.965333\t0.00884433 \t0.973333   \t0.94       \n",
      "36 \t19    \t0.963333\t0.010435   \t0.973333   \t0.94       \n",
      "37 \t18    \t0.968   \t0.00581187 \t0.973333   \t0.953333   \n",
      "38 \t18    \t0.964   \t0.0116237  \t0.973333   \t0.933333   \n",
      "39 \t15    \t0.968   \t0.00581187 \t0.973333   \t0.953333   \n",
      "40 \t18    \t0.967333\t0.0081377  \t0.973333   \t0.953333   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GAFeatureSelectionCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=True),\n",
       "                     estimator=DecisionTreeClassifier(), keep_top_k=2,\n",
       "                     n_jobs=-1, return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "\n",
    "evolved_estimator = GAFeatureSelectionCV(\n",
    "    estimator=clf,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=True,\n",
    "    keep_top_k=2,\n",
    "    elitism=True,\n",
    ")\n",
    "\n",
    "evolved_estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf554a-58aa-4a85-a6c7-f8e8a7cb94ea",
   "metadata": {},
   "source": [
    "Once the model is done, we can check which variables it chooses by using the best_features_ property, it will get an array of bools, where true means the feature at that index was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbcaff9a-a291-46ac-8568-d566d74b1998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True, False, False, False,  True, False])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evolved_estimator.best_features_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
